{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "256a06e8-5c19-493f-b597-b21467b7d585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- quantity: string (nullable = true)\n",
      "\n",
      "+--------+----------+----------+--------+\n",
      "|order_id|product_id|      date|quantity|\n",
      "+--------+----------+----------+--------+\n",
      "|       1|        31|2021-08-10|      92|\n",
      "|       2|        38|2021-08-02|      46|\n",
      "|       3|        47|2021-08-01|      48|\n",
      "|       4|        33|2021-08-09|      18|\n",
      "|       5|        29|2021-08-05|      39|\n",
      "+--------+----------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- unit_price: string (nullable = true)\n",
      "\n",
      "+----------+------------+----------+\n",
      "|product_id|product_name|unit_price|\n",
      "+----------+------------+----------+\n",
      "|         0|   product_0|        22|\n",
      "|         1|   product_1|         2|\n",
      "|         2|   product_2|         6|\n",
      "|         3|   product_3|         3|\n",
      "|         4|   product_4|        12|\n",
      "+----------+------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"Advanced_DF_EX2\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "\n",
    "# create the spark session, which is the entry point to Spark SQL engine.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "# load data\n",
    "sales_df = spark.read.format(\"csv\").option(\"header\", \"true\") \\\n",
    "       .load(\"/home/jovyan/data/sales_e1.csv\")\n",
    "sales_df.printSchema()\n",
    "sales_df.show(5)\n",
    "\n",
    "products_df = spark.read.format(\"csv\").option(\"header\", \"true\") \\\n",
    "       .load(\"/home/jovyan/data/products_e1.csv\")\n",
    "products_df.printSchema()\n",
    "products_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291c0260-b2b9-448b-8e5a-0676c4b2364d",
   "metadata": {},
   "source": [
    "**Find the best performing product in terms of the total price of the sold items for any date**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1943a2bd-8b28-46d7-9d75-48bfdd4456be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+------------+----------+-----------+\n",
      "|product_id|total_quantity|product_name|unit_price|total_price|\n",
      "+----------+--------------+------------+----------+-----------+\n",
      "|         7|        1924.0|   product_7|        15|    28860.0|\n",
      "|        15|        3075.0|  product_15|        15|    46125.0|\n",
      "|        11|        2422.0|  product_11|         4|     9688.0|\n",
      "|        29|        2868.0|  product_29|         1|     2868.0|\n",
      "|        42|        2414.0|  product_42|         3|     7242.0|\n",
      "|         3|        1859.0|   product_3|         3|     5577.0|\n",
      "|        30|        2175.0|  product_30|        12|    26100.0|\n",
      "|        34|        3582.0|  product_34|         9|    32238.0|\n",
      "|         8|        2775.0|   product_8|         5|    13875.0|\n",
      "|        28|        2270.0|  product_28|         5|    11350.0|\n",
      "+----------+--------------+------------+----------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Row(product_name='product_19', total_price=47264.0)\n",
      "product_19\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Row, Window\n",
    "\n",
    "# Calculate the number of pieces sold by each seller for each product\n",
    "sales_df_total = sales_df.groupby(col('product_id')). \\\n",
    "    agg(sum(\"quantity\").alias(\"total_quantity\"))\n",
    "\n",
    "# https://kb.databricks.com/data/join-two-dataframes-duplicated-columns.html\n",
    "# https://stackoverflow.com/questions/35258506/how-to-avoid-duplicate-columns-after-join\n",
    "joinExpression = [\"product_id\"]  #  Prevent duplicated columns when joining two DataFrames - see above links\n",
    "merged_df = sales_df_total.join(products_df, joinExpression,\"left\").withColumn(\"total_price\", col(\"total_quantity\") * col(\"unit_price\"))\n",
    "merged_df.show(10)\n",
    "print(merged_df.orderBy(col(\"total_price\").desc()).select(\"product_name\",\"total_price\").collect()[0])\n",
    "print(merged_df.orderBy(col(\"total_price\").desc()).select(\"product_name\",\"total_price\").collect()[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f37e83b-a341-45f0-b6db-8ad5dc870a43",
   "metadata": {},
   "source": [
    "**Find the best preforming product and the worst performing product in terms of the total price of the sold items for each date**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00ec39db-7d3b-440a-b09b-c5bd857b114c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- sdate: date (nullable = true)\n",
      " |-- total_quantity: double (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- unit_price: string (nullable = true)\n",
      " |-- total_price: double (nullable = true)\n",
      "\n",
      "+----------+----------+--------------+------------+----------+-----------+\n",
      "|product_id|     sdate|total_quantity|product_name|unit_price|total_price|\n",
      "+----------+----------+--------------+------------+----------+-----------+\n",
      "|         5|2021-08-05|          95.0|   product_5|        10|      950.0|\n",
      "|         2|2021-08-05|         242.0|   product_2|         6|     1452.0|\n",
      "|        33|2021-08-04|         419.0|  product_33|         9|     3771.0|\n",
      "|        10|2021-08-11|         106.0|  product_10|        10|     1060.0|\n",
      "+----------+----------+--------------+------------+----------+-----------+\n",
      "only showing top 4 rows\n",
      "\n",
      "+----------+----------+--------------+------------+----------+-----------+---------+--------+\n",
      "|product_id|     sdate|total_quantity|product_name|unit_price|total_price|rank_desc|rank_asc|\n",
      "+----------+----------+--------------+------------+----------+-----------+---------+--------+\n",
      "|        42|2021-08-01|          62.0|  product_42|         3|      186.0|       47|       1|\n",
      "|        19|2021-08-01|         437.0|  product_19|        14|     6118.0|        1|      47|\n",
      "|        36|2021-08-02|          22.0|  product_36|         3|       66.0|       48|       1|\n",
      "|        37|2021-08-02|         585.0|  product_37|        14|     8190.0|        1|      48|\n",
      "|        24|2021-08-03|           7.0|  product_24|        10|       70.0|       48|       1|\n",
      "|        15|2021-08-03|         393.0|  product_15|        15|     5895.0|        1|      48|\n",
      "|        36|2021-08-04|          29.0|  product_36|         3|       87.0|       49|       1|\n",
      "|        19|2021-08-04|         468.0|  product_19|        14|     6552.0|        1|      49|\n",
      "|        38|2021-08-05|          87.0|  product_38|         1|       87.0|       46|       1|\n",
      "|        35|2021-08-05|         483.0|  product_35|        14|     6762.0|        1|      46|\n",
      "|        39|2021-08-06|         106.0|  product_39|         2|      212.0|       48|       1|\n",
      "|        19|2021-08-06|         404.0|  product_19|        14|     5656.0|        1|      48|\n",
      "|        42|2021-08-07|          33.0|  product_42|         3|       99.0|       49|       1|\n",
      "|        37|2021-08-07|         361.0|  product_37|        14|     5054.0|        1|      49|\n",
      "|        38|2021-08-08|          74.0|  product_38|         1|       74.0|       48|       1|\n",
      "|         4|2021-08-08|         508.0|   product_4|        12|     6096.0|        1|      48|\n",
      "|        21|2021-08-09|          12.0|  product_21|         7|       84.0|       48|       1|\n",
      "|        45|2021-08-09|         440.0|  product_45|        10|     4400.0|        1|      48|\n",
      "|        43|2021-08-10|         196.0|  product_43|         1|      196.0|       49|       1|\n",
      "|        19|2021-08-10|         508.0|  product_19|        14|     7112.0|        1|      49|\n",
      "|        48|2021-08-11|         144.0|  product_48|         1|      144.0|       48|       1|\n",
      "|        15|2021-08-11|         499.0|  product_15|        15|     7485.0|        1|      48|\n",
      "+----------+----------+--------------+------------+----------+-----------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df_d = sales_df.select(\"*\", col(\"date\"),\n",
    "    to_date(col(\"date\"),\"yyyy-MM-dd\").alias(\"sdate\")).drop(\"date\")\n",
    "\n",
    "# Calculate the number of pieces sold by each seller for each product on each date\n",
    "sales_df_d_total = sales_df_d.groupby(col('product_id'), col(\"sdate\")). \\\n",
    "    agg(sum(\"quantity\").alias(\"total_quantity\"))\n",
    "\n",
    "joinExpression = [\"product_id\"] # as the two data frames use the same column id\n",
    "sales_df_d_total_price = sales_df_d_total.join(products_df, joinExpression,\"left\").withColumn(\"total_price\", col(\"total_quantity\") * col(\"unit_price\"))\n",
    "\n",
    "# Rank the product in terms of the total price, per each date. Then, select the best and worst product using ranks\n",
    "windowdesc = Window.partitionBy(col(\"sdate\")).orderBy(col(\"total_price\").desc())\n",
    "windowasc = Window.partitionBy(col(\"sdate\")).orderBy(col(\"total_price\").asc())\n",
    "\n",
    "sales_df_d_total_price.printSchema()\n",
    "sales_df_d_total_price.show(4)\n",
    "\n",
    "sales_df_d_total_windowed = sales_df_d_total_price.withColumn(\"rank_desc\", dense_rank().over(windowdesc)).withColumn(\"rank_asc\", dense_rank().over(windowasc))\n",
    "\n",
    "# Get the best and worst-performing products\n",
    "sales_df_d_total_windowed.where((col(\"rank_desc\") == 1) | (col(\"rank_asc\") == 1) ).select(\"*\").show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cff8f5-329e-44fe-8845-a1dcc29afa65",
   "metadata": {},
   "source": [
    "A useful solution for a common need\n",
    "https://stackoverflow.com/questions/34409875/how-to-get-other-columns-when-using-spark-dataframe-groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa621131-26cb-48a8-8f6b-6e01934395ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the spark context\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
